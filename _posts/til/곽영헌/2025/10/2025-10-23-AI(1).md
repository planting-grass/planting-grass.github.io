---
title: AI(1)
author: 곽영헌
date: 2025-10-23
category: TIL/곽영헌/2025/10
layout: post
---


다음은 요청하신 강의 자료의 '전략 1: 활성화 함수 선택'부터 마지막 부분까지의 핵심 내용을 요약한 것입니다.

### 전략 1: 활성화 함수 선택

 신경망에 **비선형성을 부여**하여 복잡한 패턴을 학습할 수 있도록 하는 활성화 함수의 선택은 매우 중요합니다.  

*  **ReLU (Rectified Linear Unit)**: 특별한 이유가 없다면 **우선적으로 시도**해야 할 활성화 함수입니다.    계산 효율이 높고 학습 속도가 빠르다는 장점이 있지만, '죽은 뉴런' 문제가 발생할 수 있습니다.  
*  **ReLU 개선판 (Leaky ReLU, ELU 등)**: 성능 경쟁 중이라면 ReLU의 단점을 보완한 함수들을 시도해볼 수 있습니다.    Leaky ReLU는 '죽은 뉴런' 문제를 해결하고, ELU는 ReLU의 장점을 수용하면서 평균 출력을 0에 가깝게 만듭니다.  
*  **Sigmoid, Tanh**: 이 함수들은 기울기 소실 문제와 높은 계산 비용이라는 단점을 가지고 있어, 특별한 경우(예: GAN)가 아니면 사용을 피하는 것이 좋습니다.  

### 전략 2: 데이터 전처리

*  **데이터 형식 통일**: 학습, 검증, 테스트 데이터셋 전체에 걸쳐 이미지 해상도, 색상 채널(RGB, BGR 등), 밝기, 정규화 방식을 **동일하게 적용**해야 합니다.    특히, 평균을 0, 표준편차를 1로 맞추는 정규화는 매우 중요합니다.  
*  **모델별 전처리**: AlexNet, VGG, ResNet 등 각 모델이 권장하는 고유의 전처리 방식이 있으므로 이를 확인하고 적용해야 합니다.  

### 전략 3: 모델 상수 초기화

가중치 초기화는 모델이 안정적으로 학습을 시작할 수 있도록 하는 중요한 단계입니다.

*  **0 또는 작은 무작위 값 초기화**: 모든 가중치를 0으로 초기화하면 학습이 전혀 진행되지 않습니다.    또한, 너무 작은 무작위 값으로 초기화하면 깊은 모델에서 기울기가 0으로 수렴하는 문제가 발생할 수 있습니다.  
*  **Xavier(자비에) 초기화**: 활성화 함수로 **Tanh**를 사용할 때 좋은 성능을 보입니다.  
*  **He(허) 초기화**: 활성화 함수로 **ReLU** 계열을 사용할 때 적합하며, ResNet의 저자가 제안한 방식으로 궁합이 좋습니다.  

### 전략 4: 모델 정규화 (과적합 방지)

 훈련 데이터에만 과도하게 맞춰져 새로운 데이터에 대한 성능이 떨어지는 과적합 현상을 막기 위한 전략입니다.  

* **가중치 감소 (Weight Decay)**: 가중치 크기에 패널티를 부여하여 모델의 복잡도를 제어합니다.
    *  **L2 정규화**: 가중치를 전반적으로 작게 만들어 부드러운 모델을 유도합니다.  
    *  **L1 정규화**: 중요하지 않은 가중치를 0으로 만들어 특성을 선택하는 효과가 있습니다.  
*  **드롭아웃 (Dropout)**: 학습 과정에서 무작위로 일부 뉴런을 비활성화하여 특정 뉴런에 대한 과도한 의존을 막습니다.    이는 여러 모델을 앙상블하는 것과 유사한 효과를 냅니다.  

### 전략 5: 학습 안정성 및 하이퍼파라미터 탐색

모델 학습의 안정성을 높이고 최적의 성능을 이끌어내기 위한 실질적인 기법들입니다.

*  **학습률(Learning Rate) 조정**: 학습률은 모델 성능에 가장 큰 영향을 미치는 하이퍼파라미터입니다.   학습 초기에는 큰 값으로 시작해 점차 줄여나가는 **학습률 스케줄링**이 효과적입니다. (예: 계단식, 코사인, 선형 변경)   
*  **조기 종료 (Early Stopping)**: 검증 데이터셋의 손실 값이 더 이상 개선되지 않고 오히려 증가하기 시작하면 과적합 신호로 보고 학습을 멈추는 기법입니다.  
* **하이퍼파라미터 탐색**:
    *  **탐색 방식**: 모든 조합을 시도하는 그리드 탐색보다, 무작위로 값을 선택하는 **랜덤 탐색**이 더 효율적입니다.  
    * **체계적 탐색 절차**:
        1.   **초기 손실 확인**: 초기 손실 값이 이론값(`log(클래스 수)`)과 비슷한지 확인하여 데이터 로딩 문제를 점검합니다.  
        2.   **소규모 데이터 과적합 테스트**: 작은 데이터 샘플로 의도적으로 과적합을 시도하여, 손실이 줄어드는지 확인함으로써 학습률과 초기화 방식이 적절한지 검증합니다.  
        3.   **학습률 후보군 탐색**: 로그 스케일로 다양한 학습률을 짧게(약 100회 반복) 테스트하여 손실이 효과적으로 줄어드는 후보군을 찾습니다.  
        4.   **최적 조합 탐색**: 후보 학습률과 다양한 스케줄링 조합을 1~5 에폭(epoch) 정도 짧게 학습시켜 검증 세트 성능이 좋은 조합을 선별합니다.  
        5.   **학습 곡선 분석**: 최종 후보들을 충분히 학습시키면서 학습/검증 데이터의 손실 및 정확도 곡선을 관찰하여 과적합 여부, 모델의 표현력 부족 등을 판단하고 대응합니다.  