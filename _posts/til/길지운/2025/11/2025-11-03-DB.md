---
title: 2025-11-03
author: 길지운
date: 2025-11-03
category: TIL/길지운/2025/11
layout: post
---

### 1일 1아티클
#### 데보션
###### Speculative Decoding
**정의**
- 앞으로 해야 할 행동 가이드라인에 맞게 동작 시 현실화, 이탈 시 폐기하는 방식으로 AI Agent의 Token 속도를 향상시키는 방법
- CPU의 분기 예측과 유사
  
**원리**
- 기존 `LLM` 모델은 `AutoRegressive` 방식 → 다음 단어 예측 시 한 단어씩 순차적으로 예측
- Speculative Decoding은 예측을 위해 사용할 수 있는 `LM(Large Model)`, `SM(Small Model)` 을 둠
- SM이 빠르게 다음 단어들을 예측 → SM이 예측한 다음 단어들이 자신의 예측과 맞는지 **한번에 검증**
- 아래 예시 상황에서 성능 차이 발생
  - `LM` : Next Token 예상에 10초 소요
  - `SM` : Next Token 예상에 1초 소요
  - 문장에서 4개의 단어를 예측한다고 가정
  - `LM`만 사용 : 10 * 4 = 40초
  - `SM`+`LM` 사용 : 10 + 4 = 14초
  
**한계**
- `SM`이 항상 맞을 것이라고 가정함
- `SM` 예측이 `LM` 예측과 다를 경우, **MissMatch 이후의 결과는 제거**되고 앞의 과정을 다시 반복
- 이렇게 Missmatch가 발생하여 반복하는 과정이 존재해도, `SM`과 `LM`의 성능에 따라 일반적으로 혼합 사용 시 더 효과적인 지연 감소
  
### 오늘 배운 것
1. DB
  - 모델링
  
### 내일 할 일
1. 정보처리기사 실기 준비
2. 바이브 프로젝트 회의
3. Back-end
  - 프레임워크
  
### 참고자료
- [AI Agent 속도 최적화를 위한 Speculative Decoding](https://devocean.sk.com/blog/techBoardDetail.do?ID=168001)