---
title: 2025-11-05
author: 길지운
date: 2025-11-05
category: TIL/길지운/2025/11
layout: post
---

### 1일 1아티클
#### 요즘IT
###### Chunking 최적화
**RAG와 Chunking의 중요성**
1. LLM은 실제로 기억하지 않는다
  - 대부분의 LLM은 stateless 구조
  - 이전의 대화 기록을 하나의 긴 문맥(context)으로 묶어 처리하는 것
  - `Context window`라는 한계 존재
2. 긴 문서를 다루기 위한 RAG, Chunking
  - RAG : 검색 증강 생성
  - Chunking : 긴 원본을 관리하기 쉬운 의미 있는 단위의 조각(Chunk)으로 나누는 과정
  
**Chunking 전략**
1. 고정 크기 및 재귀적 청킹
  - 고정 크기 청킹 : 단순히 고정된 글자 수나 토큰 수로 자르는 방식
  - 작게 청킹 시 의미적 정확성 증가, 문맥 파편화 발생
  - 크게 청킹 시 풍부한 문맥, 검색 성능 약화
  - 재귀적 청킹 : 문단의 끝이나 문장의 끝 등 의미가 구분되는 지점 기준으로 문서를 자르는 방식
2. 시맨틱 청킹
  - 의미 기반으로 자르는 방식
  - 문서를 개별 문장 단위로 분리 후, 각각을 임베딩하여 의미 공간의 벡터로 변환
  - 인접 문장들 간 벡터 유사도가 특정 임계값을 넘을 때 하나의 청크로 병합
3. MoC
  - 의미 기반 청킹의 진화
  - 여러 도메인별 전문가 모델 및 라우터를 통해 청킹 작업을 정교하게 진행
  - 여러 개의 큰 모델 유지 및 운영이 필요하므로 고비용
  
**Chunking 고도화를 위한 최적화 기법**
1. 메타데이터 보존
  - 청크 생성 시, 내용 + 출처 등의 문맥 정보를 메타데이터로 함께 저장
  - `청크 오버랩` : 각 청크가 이전 청크의 끝 일부분을 포함하도록 의도적으로 중복된 내용 생성
  - 청킹 시 겹치는 부분의 위치나 내용을 메타데이터에 함께 저장
2. 문맥 기반 검색
  - 청크의 주변 문맥을 요약한 짧은 문장을 인위적으로 추가
3. Late Chunking
  - 기존 방식과 달리, 전체를 통째로 임베딩한 전체 문서 임베딩 생성
  - 문서 청킹 및 개별 청크 임베딩 시, 전체 문서 임베딩 값을 주입
  
### 오늘 배운 것
  
### 내일 할 일
1. 정보처리기사 실기 준비
2. Back-end
  
### 참고자료
- [RAG 애플리케이션 개발을 위한 Chunking 최적화](https://yozm.wishket.com/magazine/detail/3432)